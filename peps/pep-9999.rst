PEP: 9999
Title: Modernizing Wheel
Author: Donald Stufft <donald@stufft.io>
Status: Draft
Type: Standards Track
Topic: Packaging
Created: 11-Feb-2026


Abstract
========

A new modernized Wheel distribution format for Python for the next 20 years.


Motivation
==========

Wheel was defined 13 years ago in :pep:`427`, and was itself an iteration on the
"Egg" file format that setuptools had designed 20+ years ago. The landscape that
we expect this format to operate in has changed significantly in that time, and
while the Wheel format has held up reasonably well, it's also struggled to keep
up with the changing landscape.

Wheel files are getting larger and larger, while the platforms that we're
expecting them to cover become increasingly diverse and varied. Resolvers are
getting more accurate, but in exchange requiring more information out of a
Wheel in order to correctly resolve while attackers are increasingly targeting
the software supply chain in increasingly sophisticated ways. The sheer scale of
PyPI has exceeded what any reasonable person would have guessed 13 or 20 years
ago.

Some of the issues that have surfaced over the years include:

* The largest wheels on PyPI are reaching 1GB in size, and often times that
  includes concentrated effort on those projects to reduce the overall size and
  even the smallest wheels can have significant overhead.
* Wheel filenames are getting longer, running into issues with operating systems
  and file systems path length limits.
* The Wheel format is incredibly malleable and suspectable to confused deputy or
  differential style attacks, where two different systems will interpret the
  same wheel differently.
* It's difficult to reliably determine if a file even *is* a wheel, other than
  by relying on out of band information (such as content in the filename).
* It's difficult to access the metadata that an installer might need to resolve
  a Wheel, without first downloading the entire wheel or relying on HTTP
  features that may not be available _and_ implementation details of the wheel
  generator.
* Even in the best case, fetching the metadata from a wheel requires at least
  one (and commonly more than one) HTTP request and downloads lots of other
  unneeded data.
* The buit in mechanisms for incrementally improving wheel have largely been
  ignored due to a worry about breaking compatability or inability to more
  selectively target a change.
* The wheel format uses the same namespace for files to be installed as it does
  for files that are part of the "structure" of the wheel, leading to the
  possibility of namespace collisions between them.
* Wheel files are not deterministic by default, and ensuring that they are fully
  deterministic requires taking great care when constructing them, making it
  harder to ship reproducable builds.
* Wheel files do not support symlinks, making it difficult to support systems
  that need the same file in multiple locations (like ``dlopen``) without
  creating multiple copies of the same file.
* Wheel files are generally considered immutable once uploaded, making it
  impossible to correct metadata issues (such as a dependency releasing a
  breaking change).

And yet, Wheel still persists, largely unchanged.

This PEP comes from taking a step back and taking a holistic view of the
problems that Wheel has faced over the years, and how those problems could be
solved from first principles.


Specification
=============

.. note:: TODO: Specify Little Endian

.. note:: TODO: Explicitly disallow emitting bytes outside of chunks.

File Structure
--------------

A Wheel 2.0 file consists of the wheel
`magic number <https://en.wikipedia.org/wiki/File_format#Magic_number>`__,
followed by a single byte version number, followed by a series of chunks.


Magic Number
++++++++++++

The first 10 bytes of a Wheel 2.0 file **MUST** contains the following
values::

    (decimal)      173 80 89 87 72 76 13 10 26 10
    (hexadecimal)  AD 50 59 57 48 4C 0D 0A 1A 0A
    (Python bytes) \xAD P Y W H L \r \n \x1A \n

This magic number indicates that the remainder of the file contains a single
wheel file.

Wheel 1.0 files **MAY** also have this magic number. If they do they **MUST**
also include the version byte. Tools **SHOULD NOT** rely on this magic number
for Wheel 1.0 files.


Version
+++++++

The 11th byte of a Wheel 2.0 file represents the version of the wheel spec that
this file conforms to.

Wheel 2.0 files **MUST** have this byte be set to ``0x02``.

Wheel 1.0 files that use the wheel magic number **MUST** have this byte set to
``0x01``.

Tools that encounter a wheel file with a magic number and version byte **MUST**
check that the version number is one that they understand, and **SHOULD**
generate an error if they do not.


Chunk Layout
++++++++++++

Each chunk consists of four parts, Length, Chunk Type, Chunk Data, and Checksum,
which is laid out in that order::

    LENGTH TYPE DATA CHECKSUM

.. note:: TODO: We need to figure out a varint encoding, LEB128? vu128? Something else?

``LENGTH`` is an unsigned integer encoded using TODO which represents the number
of bytes in the chunk's data field. This **only** counts the data field, not
itself, the chunk type, or the checksum. Zero is a valid length.

While ``LENGTH`` is a varint, and thus can be an arbitrarily large number, tools
**MAY** place their own limit on the size of this integer. If they do so, they
**MUST** keep that as an internal detail that can be adjusted by releasing a new
version of their tool (i.e. if the limit needs to grow, that should not require
anything of the end user except to upgrade their version). Tools that place a
limit **SHOULD** allow for atleast 64 bits unsigned (``u64``).

``TYPE`` is a 4 byte sequence that identifies the specific type of chunk. These
chunk types have a naming convention that makes it easier to describe and
examine wheel files, but, unless otherwise noted, tooling **MUST** treat these
4 bytes as fixed, opaque, binary values.

``DATA`` is a sequence of zero or more bytes. It **MUST** contain exactly the
number of bytes specified by ``LENGTH``, which **MAY** be zero.

.. note:: TODO: Figure out what checksum we want to use.

``CHECKSUM`` is a TODO byte sequence that stores a checksum calculated using
TODO on the preceding bytes in the chunk, including the ``TYPE`` and ``DATA``
but **not** including ``LENGTH``. This is always present, even for chunks
containing no data.

Chunks can appear in any order, unless an individual chunk type places its own
restrictions on ordering.

Multiple chunks of the same type can appear, but only if specifically permitted
for that chunk type.


Chunk Naming
++++++++++++

Chunk types are assigned so that a reader can determine some properties of a
chunk even when it does not recognize the chunk type. This allows safe, flexible
iteration of the wheel format, by allowing a reader to decide what to do when it
encounters an unknown chunk type.

This type convention **MUST** only be used by readers to interpret an unknown
chunk type. Readers that understand a chunk type **MUST** treat the type as an
opaque sequence of bytes.

``TYPE`` is a four byte sequence, and by convention the values for each byte are
limited to the ASCII upper and lowercase letters (A-Z and a-z, or 65-90 and
97-122 in decimal).

Tooling **MUST** not assign meaning to these values as letters and **MUST**
treat them as arbitrary bytes and **MUST** be prepared to handle non ASCII
values.

The third bit in each value is used as a bitflag to toggle certain properties
on or off to allow readers to understand important details about a chunk with an
unknown type.

Any bytes with undefined flag are reserved for future use and the third bit
**MUST** be set to ``0``. Readers that encounter a flag set on an undefined bit
**SHOULD NOT** complain as a meaning may be given in the future, but it should
be sufficient to treat a chunk with a reserved bit set the same way as any other
unknown chunk type.


Optional Flag
'''''''''''''

The first byte of ``TYPE`` is the "Optional" flag.

If this bit is set, then this chunk is considered optional, and readers
(particularly installers) are expected to be able to skip this chunk and still
use the wheel successfully, although likely with reduced features.

If the bit is not set, then this chunk is considered "critical", and readers
**MUST** indicate to the user that it cannot safely use this wheel.

If the same chunk type wishes to be available as both optional *and* critical,
it **MUST** be defined with two ``TYPE`` values, with this bit set and not set.


Safe to Copy Flag
'''''''''''''''''

The second byte of ``TYPE`` is the "Safe to Copy" flag. This defines the proper
handling of unrecognized chunks in a wheel file that is being modified.

If this bit is set, then this chunk is considered safe to copy to a modified
wheel file, whether or not the software recognizes the chunk type, and
regardless of the extent of the modifications that have been made to the file.

If this bit is not set, then it indicates that this chunk depends on the data
stored in other chunks. If the program has made *any* changes to non-optional
chunks, including addition, modification, deletion, or reordering, then
unrecognized unsafe chunks **MUST NOT** be copied to the new wheel. Tools
**MAY** choose to silently drop them, drop with with a warning, or abort.

If the program does recognize the chunk type, then it can ignore this bit and
choose to output an appropriately modified version.

This flag is only intended to be used for optional chunk types and will never be
set for critical chunks. Software that does not recognize a critical chunk type
**MUST** report an error and refuse to process that wheel at all.


Chunk Ordering
++++++++++++++

Critical chunks can have arbitrary ordering requirements but software cannot
work with critical chunks it doesn't understand, so software **MUST** emit
chunks based on the ordering requirements of the specific chunk type in
question.

Optional chunks cannot have arbitrary ordering requirements, because software
is not required to understand an optional chunk, therefore for an optional
chunk, the following applies:

* Unsafe to copy chunks **MAY** have ordering requirements relative to critical
  chunks.
* Safe to copy chunks **MAY** have ordering requirements relative to any
  ``DATA`` chunks.

The actual ordering rules for any particular optional chunk may be weaker.

When editing an existing wheel, the following applies:

* When copying an unknown unsafe to copy optional chunk, it **MUST NOT** be
  moved relative to any critical chunk. It **MAY** be relocated freely relative
  to other optional chunk that occur between the same pair of
  critical chunks.
* When copying an unknown safe to copy optional chunk, it **MUST NOT** be moved
* relative to any ``DATA`` chunks. It **MAY** otherwise be freely relocated.
* When copying a *known* optional chunk, software **MAY** choose to ignore the
  above general rules and follow the specific ordering rules for that type.

When reading a wheel you **MUST NOT** assume more about the positioning of any
optional chunk other than what is specified by the chunk ordering rules. In
particular, you **MUST NOT** assume that a specific optional chunk occurs with
any particular position relative to other optional chunks.


Rationale
=========

Magic Number
------------

The magic number sequence was adapted from
`PNG's file signature <https://www.libpng.org/pub/png/spec/1.2/PNG-Structure.html#PNG-file-signature>`__.

This magic number both identifies the file as a Wheel file and provides
immediate detection of common file transfer problems.

The first two bytes distinguishes wheel files on systems that expect the first
two bytes to identify the file type uniquely.

The first byte is chosen as a non text character in ASCII and UTF8 nor is it
used as the first byte in any of the byte order makes for any UTF encoding. When
decoding it as ``latin1``, commonly used to smuggles arbitrary bytes through
text, it is represented as a
`soft hyphen <https://en.wikipedia.org/wiki/Soft_hyphen>`__, which is an
uncommonly used glyph. This reduces the probability that a text file may be
misrecognized as a wheel file and also catches bad file transfers that clear the
final bit.

Bytes two through six name the format as ``PYWHL``.

The CR LF sequence catches bad file transfers that alter new line sequences.

The Ctrl-Z chracter stops file display under some terminals, preventing a screen
full of gibberish bytes if a wheel file is accidentally piped to the terminal.

The final LF checks for the inverse of the CR LF translation problem.


Version Number
--------------

The version number is added immediately after the magic number as that controls
how the rest of the file should be interpreted.

It's unlikely that we'll ever *need* a Wheel 3.0, given the ability to evolve
the spec through new chunks (if we wanted to "revert" back to a Wheel 1.0 zip
file, that could just be a type of chunk for instance that replaces existing
chunks). However we still include a versioning byte because a single  byte is a
pretty small cost to pay for the flexibility to do that if we needed to in the
future.

Adding that versioning byte also means that it's pretty trivial to apply some
of the benefits of this spec to Wheel 1.0 by prefixing a Wheel 1.0 file with the
magic number and versioning byte (set to ``0x01``). It's expected that most
tools will not be able to rely on that existing and will have to continue to
rely on out of band information or trying to guess if a file is a wheel, but
tools can emit that immediately with no problems, and in the future PyPI may be
able to reject new uploads, even for Wheel 1.0 files, that do not include the
magic number and versioning byte.

Using a single byte means that we're limited to 255 versions. That should not be
an issue given we should not need to ever release a new version at all. Even if
we do release new versions, if we continue our pace of a new version every ~10
years, a single byte gets us over 2500 years of versions, and even if we
increase pace, at worst case we can define ``0xFF`` to mean the next byte needs
to be read as well.


Chunk Layout
------------

The chunk layout was adapted from
`PNG's chunk layout <https://www.libpng.org/pub/png/spec/1.2/PNG-Structure.html#Chunk-layout>`__.

This design makes it easy for a tool that is reading a wheel file to skip
unrecognized or uninteresting chunks by reading the ``LENGTH`` field and then
skipping the appropiate number of bytes.

.. note:: TODO: Depending on checksum strategy, "skipping" may not be exactly correct.

A separate checksum is provided for each chunk in order to detect badly
transferred wheel files as quickly as possible.

The chunk length is excluded from the checksum so that the checksum can be
calculated as the data is generated which avoids a second pass over the data in
cases where the chunk length is not known in advance. This does not create any
risk of failing to discover corruption or tampering, since if the length is
wrong then the checksum will be fed the wrong number of bytes and be compared
against the wrong bytes, causing the checksum to fail anyways.

--
The convention to limit naming to upper and lowercase ASCII letters means that
toggling that third bit on or off changes the letter between upper and lower
case. This is intended to make it easier for a human to describe and interpret
a block.

As an example, the ASCII letter "A" is made up of these 8 bits ``01000001``, and
if we toggle the third bit from ``0`` to ``1``, we get ``01100001`` or the ASCII
letter "a".

Within the ASCII letter convention, this means that the first letter of the tag
indicates whether the chunk is mandatory (uppercase letter) or optional
(lowercase).


Chunk Naming
------------

The chunk naming rules were adapted from
`PNG's chunk naming <https://www.libpng.org/pub/png/spec/1.2/PNG-Structure.html#Chunk-naming-conventions>`__.

These naming rules allow safe, flexible extension and iteration of the wheel
format. It will *hopefully* work better than version numbers because it works on
a feature by feature basis rather than being an overall indicator.

Version numbers fall short because they apply even in cases where an individual
wheel does not use that new feature and *could* be represented using the old
version. This leads to tooling having to choose whether to emit the newest
version by default (and possibly break users for no reason) or trying to pick
the oldest version that supports all of the features that an individual wheel
actually uses.

Readers can process newer wheels if and only if that wheel uses no unknown
critical features (as indicated by finding unknown critical chunks). Unknown
optional chunks can be safely ignored.

Tools that produce wheels don't have to make any decisions about what version
they're emitting or try to track what the minimum version needed is or anything
like that, it just naturally happens by using the features.

Unlike PNG we do not support "private" extensions. We could but given the nature
of wheels that feels like something that is unlikely to be a useful thing (for
example, PyPI would likely have to block uploads that use private extensions).

We allow modification of wheel files (using tools like auditwheel) in a safe way
even if they use newer features that the auditwheel tool doesn't understand.

Not all possible modification scenarios are covered by the safe/unsafe
semantics, in particular chunks that are dependent on the total file contents
including optional chunks. Definition of such chunks is discouraged, but it can
be done if those "total file contents" chunks are defined as critical chunks.

In general, optional chunks can depend on critical chunks, but not on other
optional chunks.


Chunk Ordering
--------------

We want to support tooling to be able to modify existing wheels without having
to understand every chunk type used within that wheel, but to do that we need
to define what rules are in place for how chunks are ordered, even if
hypothetically, that definition was that chunks can be freely reordered so you
can not assume anything about the order of chunks.

Critical chunks are easiest, it's an error to read or write wheel files that
contain unknown critical chunks, so by definition every tool knows what rules
are in place for critical chunks, and each chunk type can dictate arbitrary
ordering rules.

Optional chunks are harder, because tools are not required to support them, but
we also don't want to have them just drop any optional chunk type they don't
understand as most of the time they can be handled just fine with some basic
rules.

Our rules then give us some broad strokes that attempt to balance between not
being able to assume *anything* about the ordering of optional chunks while
still being able to modify the wheel file (e.g. to insert your own chunks).

The chunks that are unsafe to copy being limited to maintaining their relative
position to any critical chunk means that if their data depends on what's in
those critical chunks (which is likely given they've been flagged as unsafe to
copy) their semantics can mean they "act upon" any critical chunk that came
before it (or alternatively came after it) without worrying about losing that
meaning. It's not allowed to retain an unsafe to copy chunk while making any
changes to critical chunks, so we also know that that optional chunk isn't going
to accedentally apply (or not be applied) to an unexpected set of critical
chunks.

Of course this unsafe to copy chunk may not actually care about its relative
position (perhaps it summarizes all critical chunks together or something), but
in that case it won't be harmed by maintaining the relative position.

We know that safe to copy chunks don't depend on the data in any other chunk so
they are safe to freely copy, so we could allow safe to copy chunks to be freely
reordered throughout the entire file. However, unless the wheel file is empty,
we know it's going to have at least one ``DATA`` chunk, those ``DATA`` chunks
are likely going to be the largest chunks in a wheel, and maintaining relative
position to them allows storing chunks that you'd really rather read before you
get to all of the ``DATA`` before the ``DATA`` portions.

We otherwise don't allow depending on ordering of optional chunks relative to
other optional chunks, because otherwise it wouldn't be possible to insert new
optional chunks at all if there were any unknown chunk types in the file.


Backwards Compatibility
=======================


Security Implications
=====================


How to Teach This
=================


Reference Implementation
========================


Rejected Ideas
==============


Open Issues
===========

Magic Number "Name"
-------------------

As far as I know, there are no other file types that begin with ``b"\x9bP"``,
however that means that our 2 byte discriminator includes ``P`` because our
ASCII tag is ``PYWHL``, and if we ever want to extend this to sdists, then we
couldn't use ``PYSDIST`` without having the same first two bytes.

I'm not sure how many systems out there use the first two bytes (PNG just
mentions it but doesn't indicate which systems those are) and whether or not we
care.

We could just shorten the name to be ``WHL`` which saves 2 bytes and fixes the
problem, but I kinda liked that ``PYWHL`` disambiguates this from any other
kind of wheel... but I'm not aware of any other kind of wheel and our file
extension is ``.whl`` so it's probably fine? Then if we ever did something like
this for sdist it could be ``SDIST``, though that feels overlly generic, but
isn't something we'd have to fix today?

We could also do ``WHLPY`` or something, but that reads "wrong" to me.


Checksum Algorithm
------------------

The PNG format uses CRC, which is old and tried/true and already has fast-ish
code in the Python standard library to compute it.

That being said, CRC32 only provides 32 bits of collision resistance which
means that an attacker could construct malformed wheel files that collide in
interesting ways, and
`CRC32 isn't particularly fast <https://jolynch.github.io/posts/use_fast_data_algorithms/>`__.

A better, more modern option if we want to stick with non-cryptographic hashes
would be `xxhash <https://xxhash.com/>`__, (specifically XXH3 would be ideal)
however that doesn't exist in the Python stdlib so we'd need either a pure
Python option that is "fast enough" for pip and/or we'd need to get it into the
Python standard library. That might be hard since there's not a generic
interface for adding non-crytographic hashes to the stdlib (crc32 is brought in
as part of the ``zlib`` library).

Another option would be to use a crytographic hash to provide more resistance
to tampering. This is, in theory, slower, but modern hash algorithms can be
pretty fast (likely faster than CRC32 implemented in Python, but we should
benchmark it). Something like `blake3 <https://github.com/BLAKE3-team/BLAKE3/>`__
would be ideal, but it's not guaranteed to exist in the stdlib ``hashlib``
so we would need to get it included for pip. The `blake2 <https://www.blake2.net/>`__
algorithm is guaranteed in ``hashlib`` so that *could* be used, but blake3 would
be better.

Personally, I think blake3 seems like a really good choice here. In theory we
don't *need* a crytographic hash, as this checksum is mostly for detecting
transfer issues or bitrot, but stronger hashes makes it harder for attackers to
attack systems by creating polyglot or other types of filr format attacks.


Checksum Chaining
-----------------

The PNG format only implements checksumming for individual chunks, which means
it won't detect issues where entire chunks are missing or out of order (unless
the ordering is wholly invalid in the spec).

One option to strengthen this would be to compute the checksum by chaining them
or computing a "rolling" checksum.

A rolling checksum would take advantage of the fact that checksumming allows you
to get the current checksum, without throwing away the internal state so that
you can continue to add additional data to it over time. Meaning that each
chunk's checksum is actually a checksum of ``TYPE DATA`` for itself *and* all
previous chunks.

The downside to that would be that it's impossible to fully skip a chunk, as
you'd *have* to read the ``TYPE`` and ``DATA`` bytes and feed them into the
checksumming algorithm in order to properly have the correct state.

The upside is it makes computing the checksums for the entire file more
ergonomic as you're only dealing with a single checksum state, you're just
essentially emitting check points at each chunk.

Chaining checksums is similiar, except instead of having the checksum be the
content of all of the chunks ``TYPE DATA`` up to that point, each chunk is still
checksummed mostly independently, but it is seeded in some way (typically by
using a keyed hash) by the checksum of the previous chunk.

The downside to this is computing the checksum for the whole file is a little
more complicated and less ergonomic and you're limited either to algorithms that
support keyed hashing or you have to use HMAC or invent your own scheme.

The upside is that skipping chunks without reading the bytes still mostly works,
you just have to also read the ``CHECKSUM`` bytes from the previous chunk.

Personally I think we should implement something to extend the checksum across
all chunks, as that makes it much harder to tamper or produce malformed files
and it also has the really nice property that the checksum on the final chunk
also acts as a checksum for the entire file (mostly, it doesn't include the
lengths but that should be fine as if the lengths are wrong the checksum will
still fail).

I lean towards using chaining, particularly if we end up using blake3 or another
hash that natively supports keyed hashing, since that still allows for
efficiently skipping blocks.


Structure of Data
=================

.. note:: TODO: If we change this, we might have to change the ordering rules since
          they assumed a ``DATA`` chunk type.

We can structure records such that there's a "File Record" chunk that points to
a "File Data" chunk, which allows us to build a sort of index of files at the
front (or end if we want) of the archive.

This makes it easy for us to tweak things so that files can either be compressed
together or individually. However, it makes it harder to compress the file
metadata itself (it can be done, but will have less compression than if the
compression operates over everything).

It also makes chunk ordering more important, as all of the file records have to
come first before the data (orwe need to implement seeking around) and that can
cause problems, but it does make getting a list of files much faster.

We could have individual chunks that hold file metadata + file data, one per file,
which keeps each chunk self contained, but means that we're back at individual
compression unless we come up with a scheme that allows chaining multiple chunks
together for compression.

This could function similiarly to the "rolling checksum" option, where you have
to

Another option besides chaining multiple files together, could be a dictionary
chunk that allows sharing a compression dictionary for multiple files.


Acknowledgements
================


Appendix: Zip Format
====================

The existing wheel format is defined as a zip file, and an astute observer could
notice that a number of the issues spelled out are not necessarily problems with
the wheel format itself, but rather are problems that wheel inherits from zip.

The zip format is 36 years old, and like wheel the world is a vastly different
place than when it was first created, and over the years it has also grown a
large number of warts in an attempt to keep up with the world, and a lot (but
not all) of the issues with zip stem from those warts.

Without delving into the full zip file format, a rough idea of it is that it's
basically a series of file records that consistent of ``HEADER + DATA + TRAILER``,
with the ``HEADER`` containing metadata about the file (name, size, etc), ``DATA``
containing the actual bytes of the file, and ``TRAILER`` also containing some
duplicate metadata from ``HEADER`` (to support cases where that information
isn't known in advance and you have to write the ``DATA`` to disk first).

After all of the file records, there's a "central record", which is basically an
index of all of the files in the zip file, along with the information needed to
locate the "real" file record that contains the data.

The cannonical way to read a zip file is to seek backwards until you locate the
magic bytes that indicate the end of the central record, and then read that to
get the start of the central record, seek to there, and then read forwards again
from that location to get an index of all of the files, and then use that index
to jump to the correct file record and actually get the needed data.

This has some interesting properties that enables some neat (and sometimes
useful) tricks. For example, a zip file tolerates an arbitrary amount of garbage
prepended to it, which means you can create a self extracting zip file by
prepending a binary that reads itself as a zip file and extracts itself.

Unfortunately this also has some very serious problems.

Arbitrary garbage can be prepended to your zip file, so that means you can
create polyglot files that read as both a zip and something else (which is what
the self extracting zip file does!) and use that to smuggle a malicious payload
into contexts that it might otherwise not be able to get through.

One of the most fundamental aspects of pretty much any sort of data stream is
the ability to read it from front to back, but it's much more rare to be able to
read it backwards-- and generally involves having to make guesses about how big
of a chunk you need to read in order to hopefully get the magic bytes that you
need.

To make matters worse, zip files support an "archive comment", which in theory
is an unbounded length byte string that appears at the very end of the central
record (i.e. the final bytes of the zip file are the comment). The zip format
recommends that comments be limited to 64k bytes, so that acts as a sort of
cap on the maximum number of bytes you need to read (but the spec doesn't
*require* limiting it to 64k). Worse though, is that since the archive comment
can be arbitrary bytes, you can put another zip file in those comments and try
to confuse two different zip parsers around what zip file they're attempting
to parse.

Zip files also have, at minimum, around 100 bytes of overhead per file stored
to deal with all of the related metadata (not counting the actual contents of
the filename and if there are any file comments). Aboout a third of that
overhead is a complete duplicate of another third, just stored in different
locations.

Zip files use fixed width integers everywhere, including for things like size
of a file, which means that natively there's a limit on how large of a file can
be stored in a zip. To address this, a standard "Zip64" extension was added
which allows a larger set of fixed width integers, but comes at the cost of
introducing two more duplicates of some of that existing data.

This duplication also means that the same data can be read from multiple places
and if they disagree it's not clear which data is correct. Different parsers
may end up selecting different sources of data to indicate what is correct.

The central record at the end of the file pointing back to locations earlier
in the stream also means that you can have the same file represented multiple
times in the stream, with only a single record in the central record or even
no record. This allows you to update a file by emitting it a second (or third)
time and only pointing the central record at the final one, or deleting a file
by just omitting it from the central record.

Which means that on top of garbage being able to be prepended and appended to a
zip file, you can also just inject it in the middle as well, and yet another
chance for a confused deputy style attack if parsers get confused.

Compression in zip also only operates file by file, so you get a much worse
compression ratio than if you compressed files as one large chunk in exchange
for more efficient random access of files.

A lot of these behaviors make sense in the context of when zip was designed,
nearly 40 years ago a zip file might have to span across multiple floppy disks,
and if you were 10 disks in and realized a file in the first disk needed updated,
it's a lot nicer to just keep going and update the central record than have to
restart over and you can find all of the files from the central record on your
last disk.

This 40 year history also means that there's a lot of cruft in the format.
Timestamps are limited to 2 second resolution because 40 years ago MS-DOS
timestamps were limited to 2 second resolution, some of the overhead is where
the "disk number" is intended to be stored so you know what disk you need to
insert to read a given file, etc.

Ultimately, the vast majority of what zip format gives us, isn't actually that
useful or good. Where the zip format really shines for us is that you can use
any old zip program to inspect a wheel and see what's inside it, debug it, etc.



Footnotes
=========


Copyright
=========

This document is placed in the public domain or under the
CC0-1.0-Universal license, whichever is more permissive.
