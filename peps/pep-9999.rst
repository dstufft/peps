PEP: 9999
Title: Modernizing Wheel
Author: Donald Stufft <donald@stufft.io>
Status: Draft
Type: Standards Track
Topic: Packaging
Created: 11-Feb-2026


Abstract
========

A new modernized Wheel distribution format for Python for the next 20 years.


Motivation
==========

Wheel was defined 13 years ago in :pep:`427`, and was itself an iteration on the
"Egg" file format that setuptools had designed 20+ years ago. The landscape that
we expect this format to operate in has changed significantly in that time, and
while the Wheel format has held up reasonably well, it's also struggled to keep
up with the changing landscape.

Wheel files are getting larger and larger, while the platforms that we're
expecting them to cover become increasingly diverse and varied. Resolvers are
getting more accurate, but in exchange requiring more information out of a
Wheel in order to correctly resolve while attackers are increasingly targeting
the software supply chain in increasingly sophisticated ways. The sheer scale of
PyPI has exceeded what any reasonable person would have guessed 13 or 20 years
ago.

Some of the issues that have surfaced over the years include:

* The largest wheels on PyPI are reaching 1GB in size, and often times that
  includes concentrated effort on those projects to reduce the overall size and
  even the smallest wheels can have significant overhead.
* Wheel filenames are getting longer, running into issues with operating systems
  and file systems path length limits.
* The Wheel format is incredibly malleable and suspectable to confused deputy or
  differential style attacks, where two different systems will interpret the
  same wheel differently.
* It's difficult to reliably determine if a file even *is* a wheel, other than
  by relying on out of band information (such as content in the filename).
* It's difficult to access the metadata that an installer might need to resolve
  a Wheel, without first downloading the entire wheel or relying on HTTP
  features that may not be available _and_ implementation details of the wheel
  generator.
* Even in the best case, fetching the metadata from a wheel requires at least
  one (and commonly more than one) HTTP request and downloads lots of other
  unneeded data.
* The buit in mechanisms for incrementally improving wheel have largely been
  ignored due to a worry about breaking compatability or inability to more
  selectively target a change.
* The wheel format uses the same namespace for files to be installed as it does
  for files that are part of the "structure" of the wheel, leading to the
  possibility of namespace collisions between them.
* Wheel files are not deterministic by default, and ensuring that they are fully
  deterministic requires taking great care when constructing them, making it
  harder to ship reproducable builds.
* Wheel files do not support symlinks, making it difficult to support systems
  that need the same file in multiple locations (like ``dlopen``) without
  creating multiple copies of the same file.
* Wheel files are generally considered immutable once uploaded, making it
  impossible to correct metadata issues (such as a dependency releasing a
  breaking change).

And yet, Wheel still persists, largely unchanged.

This PEP comes from taking a step back and taking a holistic view of the
problems that Wheel has faced over the years, and how those problems could be
solved from first principles.


Specification
=============

.. note:: TODO: Specify Little Endian

.. note:: TODO: Explicitly disallow emitting bytes outside of chunks.

File Structure
--------------

A Wheel 2.0 file consists of the wheel
`magic number <https://en.wikipedia.org/wiki/File_format#Magic_number>`__,
followed by a single byte version number, followed by a series of chunks.


Magic Number
++++++++++++

The first 10 bytes of a Wheel 2.0 file **MUST** contains the following
values::

    (decimal)      173 80 89 87 72 76 13 10 26 10
    (hexadecimal)  AD 50 59 57 48 4C 0D 0A 1A 0A
    (Python bytes) \xAD P Y W H L \r \n \x1A \n

This magic number indicates that the remainder of the file contains a single
wheel file.

Wheel 1.0 files **MAY** also have this magic number. If they do they **MUST**
also include the version byte. Tools **SHOULD NOT** rely on this magic number
for Wheel 1.0 files.


Version
+++++++

The 11th byte of a Wheel 2.0 file represents the version of the wheel spec that
this file conforms to.

Wheel 2.0 files **MUST** have this byte be set to ``0x02``.

Wheel 1.0 files that use the wheel magic number **MUST** have this byte set to
``0x01``.

Tools that encounter a wheel file with a magic number and version byte **MUST**
check that the version number is one that they understand, and **SHOULD**
generate an error if they do not.


Chunk Layout
++++++++++++

Each chunk consists of four parts, Length, Chunk Type, Chunk Data, and Checksum,
which is laid out in that order::

    LENGTH TYPE DATA CHECKSUM

.. note:: TODO: We need to figure out a varint encoding, LEB128? vu128? Something else?

``LENGTH`` is an unsigned integer encoded using TODO which represents the number
of bytes in the chunk's data field. This **only** counts the data field, not
itself, the chunk type, or the checksum. Zero is a valid length.

While ``LENGTH`` is a varint, and thus can be an arbitrarily large number, tools
**MAY** place their own limit on the size of this integer. If they do so, they
**MUST** keep that as an internal detail that can be adjusted by releasing a new
version of their tool (i.e. if the limit needs to grow, that should not require
anything of the end user except to upgrade their version). Tools that place a
limit **SHOULD** allow for atleast 64 bits unsigned (``u64``).

``TYPE`` is a 4 byte sequence that identifies the specific type of chunk. These
chunk types have a naming convention that makes it easier to describe and
examine wheel files, but, unless otherwise noted, tooling **MUST** treat these
4 bytes as fixed, opaque, binary values.

``DATA`` is a sequence of zero or more bytes. It **MUST** contain exactly the
number of bytes specified by ``LENGTH``, which **MAY** be zero.

.. note:: TODO: Figure out what checksum we want to use.

``CHECKSUM`` is a TODO byte sequence that stores a checksum calculated using
TODO on the preceding bytes in the chunk, including the ``TYPE`` and ``DATA``
but **not** including ``LENGTH``. This is always present, even for chunks
containing no data.

Chunks can appear in any order, unless an individual chunk type places its own
restrictions on ordering.

Multiple chunks of the same type can appear, but only if specifically permitted
for that chunk type.


Chunk Naming
++++++++++++

Chunk types are assigned so that a reader can determine some properties of a
chunk even when it does not recognize the chunk type. This allows safe, flexible
iteration of the wheel format, by allowing a reader to decide what to do when it
encounters an unknown chunk type.

This type convention **MUST** only be used by readers to interpret an unknown
chunk type. Readers that understand a chunk type **MUST** treat the type as an
opaque sequence of bytes.

``TYPE`` is a four byte sequence, and by convention the values for each byte are
limited to the ASCII upper and lowercase letters (A-Z and a-z, or 65-90 and
97-122 in decimal).

Tooling **MUST** not assign meaning to these values as letters and **MUST**
treat them as arbitrary bytes and **MUST** be prepared to handle non ASCII
values.

The third bit in each value is used as a bitflag to toggle certain properties
on or off to allow readers to understand important details about a chunk with an
unknown type.

Any bytes with undefined flag are reserved for future use and the third bit
**MUST** be set to ``0``. Readers that encounter a flag set on an undefined bit
**SHOULD NOT** complain as a meaning may be given in the future, but it should
be sufficient to treat a chunk with a reserved bit set the same way as any other
unknown chunk type.


Optional Flag
'''''''''''''

The first byte of ``TYPE`` is the "Optional" flag.

If this bit is set, then this chunk is considered optional, and readers
(particularly installers) are expected to be able to skip this chunk and still
use the wheel successfully, although likely with reduced features.

If the bit is not set, then this chunk is considered "critical", and readers
**MUST** indicate to the user that it cannot safely use this wheel.

If the same chunk type wishes to be available as both optional *and* critical,
it **MUST** be defined with two ``TYPE`` values, with this bit set and not set.


Safe to Copy Flag
'''''''''''''''''

The second byte of ``TYPE`` is the "Safe to Copy" flag. This defines the proper
handling of unrecognized chunks in a wheel file that is being modified.

If this bit is set, then this chunk is considered safe to copy to a modified
wheel file, whether or not the software recognizes the chunk type, and
regardless of the extent of the modifications that have been made to the file.

If this bit is not set, then it indicates that this chunk depends on the data
stored in other chunks. If the program has made *any* changes to non-optional
chunks, including addition, modification, deletion, or reordering, then
unrecognized unsafe chunks **MUST NOT** be copied to the new wheel. Tools
**MAY** choose to silently drop them, drop with with a warning, or abort.

If the program does recognize the chunk type, then it can ignore this bit and
choose to output an appropriately modified version.

This flag is only intended to be used for optional chunk types and will never be
set for critical chunks. Software that does not recognize a critical chunk type
**MUST** report an error and refuse to process that wheel at all.


Chunk Ordering
++++++++++++++

Critical chunks can have arbitrary ordering requirements but software cannot
work with critical chunks it doesn't understand, so software **MUST** emit
chunks based on the ordering requirements of the specific chunk type in
question.

Optional chunks cannot have arbitrary ordering requirements, because software
is not required to understand an optional chunk, therefore for an optional
chunk, the following applies:

* Unsafe to copy chunks **MAY** have ordering requirements relative to critical
  chunks.
* Safe to copy chunks **MAY** have ordering requirements relative to any
  ``DATA`` chunks.

The actual ordering rules for any particular optional chunk may be weaker.

When editing an existing wheel, the following applies:

* When copying an unknown unsafe to copy optional chunk, it **MUST NOT** be
  moved relative to any critical chunk. It **MAY** be relocated freely relative
  to other optional chunk that occur between the same pair of
  critical chunks.
* When copying an unknown safe to copy optional chunk, it **MUST NOT** be moved
* relative to any ``DATA`` chunks. It **MAY** otherwise be freely relocated.
* When copying a *known* optional chunk, software **MAY** choose to ignore the
  above general rules and follow the specific ordering rules for that type.

When reading a wheel you **MUST NOT** assume more about the positioning of any
optional chunk other than what is specified by the chunk ordering rules. In
particular, you **MUST NOT** assume that a specific optional chunk occurs with
any particular position relative to other optional chunks.


Rationale
=========

Custom File Format
------------------

A number of the issues with Wheel 1.0 are not issues related to Wheel 1.0
itself, but rather are issues that have been inherited from the zip file format,
which makes it difficult to actually fix those issues, limiting us to only being
able to work around them. It can actually be impossible to avoid some of them,
because the work around would ultimately be "don't use this feature", but of
course a tool that is trying to read an arbitrary wheel file has no control over
what features of zip that wheel has chosen to use or may require using a custom
zip parser that allows access to the internals in ways most parsers do not.

The zip format is a very general purpose format, which means that it includes a
large amount of flexibility and features that are not relevant to this use case.
It's also lead the zip format to make choices about trade offs that can make
more sense in a general purpose format, but where those trade offs choices are
worse for wheels.

This zip format is also a very old format, nearly 40 years old, and the world
looks very different than it did 40 years ago, which brings a lot of cruft in
the zip format either to support scenarios that no longer exist or to bolt on
support for something that the original design never could have imagined
needing.

Ultimately the things that we need from our format are simple enough that
writing our own archive format is not a particularly large undertaking, and
affords us the ability to make choices that are best for our particular use
case. The majority of users who will be working with these files will be using
one of a small handful of tools in a limited number of programming languages, so
providing libraries for those tools in those languages is not a massive
undertaking either.

The biggest ongoing cost to using a custom format over zip is losing the ability
to introspect and debug a wheel file with generic zip tools that are nearly
universally available. This can be mitigated by providing our own tooling to do
that, which may end up providing a nicer experience anyways, as we can tailor
that tooling to our specific use cases.

For more information on the issues around zip files, see
`Appendix: Zip Format`_.


Magic Number
------------

The magic number sequence was adapted from
`PNG's file signature <https://www.libpng.org/pub/png/spec/1.2/PNG-Structure.html#PNG-file-signature>`__.

This magic number both identifies the file as a Wheel file and provides
immediate detection of common file transfer problems.

The first two bytes distinguishes wheel files on systems that expect the first
two bytes to identify the file type uniquely.

The first byte is chosen as a non text character in ASCII and UTF8 nor is it
used as the first byte in any of the byte order makes for any UTF encoding. When
decoding it as ``latin1``, commonly used to smuggles arbitrary bytes through
text, it is represented as a
`soft hyphen <https://en.wikipedia.org/wiki/Soft_hyphen>`__, which is an
uncommonly used glyph. This reduces the probability that a text file may be
misrecognized as a wheel file and also catches bad file transfers that clear the
final bit.

Bytes two through six name the format as ``PYWHL``.

The CR LF sequence catches bad file transfers that alter new line sequences.

The Ctrl-Z chracter stops file display under some terminals, preventing a screen
full of gibberish bytes if a wheel file is accidentally piped to the terminal.

The final LF checks for the inverse of the CR LF translation problem.


Version Number
--------------

The version number is added immediately after the magic number as that controls
how the rest of the file should be interpreted.

It's unlikely that we'll ever *need* a Wheel 3.0, given the ability to evolve
the spec through new chunks (if we wanted to "revert" back to a Wheel 1.0 zip
file, that could just be a type of chunk for instance that replaces existing
chunks). However we still include a versioning byte because a single  byte is a
pretty small cost to pay for the flexibility to do that if we needed to in the
future.

Adding that versioning byte also means that it's pretty trivial to apply some
of the benefits of this spec to Wheel 1.0 by prefixing a Wheel 1.0 file with the
magic number and versioning byte (set to ``0x01``). It's expected that most
tools will not be able to rely on that existing and will have to continue to
rely on out of band information or trying to guess if a file is a wheel, but
tools can emit that immediately with no problems, and in the future PyPI may be
able to reject new uploads, even for Wheel 1.0 files, that do not include the
magic number and versioning byte.

Using a single byte means that we're limited to 255 versions. That should not be
an issue given we should not need to ever release a new version at all. Even if
we do release new versions, if we continue our pace of a new version every ~10
years, a single byte gets us over 2500 years of versions, and even if we
increase pace, at worst case we can define ``0xFF`` to mean the next byte needs
to be read as well.


Chunk Layout
------------

The chunk layout was adapted from
`PNG's chunk layout <https://www.libpng.org/pub/png/spec/1.2/PNG-Structure.html#Chunk-layout>`__.

This design makes it easy for a tool that is reading a wheel file to skip
unrecognized or uninteresting chunks by reading the ``LENGTH`` field and then
skipping the appropiate number of bytes.

.. note:: TODO: Depending on checksum strategy, "skipping" may not be exactly correct.

A separate checksum is provided for each chunk in order to detect badly
transferred wheel files as quickly as possible.

The chunk length is excluded from the checksum so that the checksum can be
calculated as the data is generated which avoids a second pass over the data in
cases where the chunk length is not known in advance. This does not create any
risk of failing to discover corruption or tampering, since if the length is
wrong then the checksum will be fed the wrong number of bytes and be compared
against the wrong bytes, causing the checksum to fail anyways.

--
The convention to limit naming to upper and lowercase ASCII letters means that
toggling that third bit on or off changes the letter between upper and lower
case. This is intended to make it easier for a human to describe and interpret
a block.

As an example, the ASCII letter "A" is made up of these 8 bits ``01000001``, and
if we toggle the third bit from ``0`` to ``1``, we get ``01100001`` or the ASCII
letter "a".

Within the ASCII letter convention, this means that the first letter of the tag
indicates whether the chunk is mandatory (uppercase letter) or optional
(lowercase).


Chunk Naming
------------

The chunk naming rules were adapted from
`PNG's chunk naming <https://www.libpng.org/pub/png/spec/1.2/PNG-Structure.html#Chunk-naming-conventions>`__.

These naming rules allow safe, flexible extension and iteration of the wheel
format. It will *hopefully* work better than version numbers because it works on
a feature by feature basis rather than being an overall indicator.

Version numbers fall short because they apply even in cases where an individual
wheel does not use that new feature and *could* be represented using the old
version. This leads to tooling having to choose whether to emit the newest
version by default (and possibly break users for no reason) or trying to pick
the oldest version that supports all of the features that an individual wheel
actually uses.

Readers can process newer wheels if and only if that wheel uses no unknown
critical features (as indicated by finding unknown critical chunks). Unknown
optional chunks can be safely ignored.

Tools that produce wheels don't have to make any decisions about what version
they're emitting or try to track what the minimum version needed is or anything
like that, it just naturally happens by using the features.

Unlike PNG we do not support "private" extensions. We could but given the nature
of wheels that feels like something that is unlikely to be a useful thing (for
example, PyPI would likely have to block uploads that use private extensions).

We allow modification of wheel files (using tools like auditwheel) in a safe way
even if they use newer features that the auditwheel tool doesn't understand.

Not all possible modification scenarios are covered by the safe/unsafe
semantics, in particular chunks that are dependent on the total file contents
including optional chunks. Definition of such chunks is discouraged, but it can
be done if those "total file contents" chunks are defined as critical chunks.

In general, optional chunks can depend on critical chunks, but not on other
optional chunks.


Chunk Ordering
--------------

We want to support tooling to be able to modify existing wheels without having
to understand every chunk type used within that wheel, but to do that we need
to define what rules are in place for how chunks are ordered, even if
hypothetically, that definition was that chunks can be freely reordered so you
can not assume anything about the order of chunks.

Critical chunks are easiest, it's an error to read or write wheel files that
contain unknown critical chunks, so by definition every tool knows what rules
are in place for critical chunks, and each chunk type can dictate arbitrary
ordering rules.

Optional chunks are harder, because tools are not required to support them, but
we also don't want to have them just drop any optional chunk type they don't
understand as most of the time they can be handled just fine with some basic
rules.

Our rules then give us some broad strokes that attempt to balance between not
being able to assume *anything* about the ordering of optional chunks while
still being able to modify the wheel file (e.g. to insert your own chunks).

The chunks that are unsafe to copy being limited to maintaining their relative
position to any critical chunk means that if their data depends on what's in
those critical chunks (which is likely given they've been flagged as unsafe to
copy) their semantics can mean they "act upon" any critical chunk that came
before it (or alternatively came after it) without worrying about losing that
meaning. It's not allowed to retain an unsafe to copy chunk while making any
changes to critical chunks, so we also know that that optional chunk isn't going
to accedentally apply (or not be applied) to an unexpected set of critical
chunks.

Of course this unsafe to copy chunk may not actually care about its relative
position (perhaps it summarizes all critical chunks together or something), but
in that case it won't be harmed by maintaining the relative position.

We know that safe to copy chunks don't depend on the data in any other chunk so
they are safe to freely copy, so we could allow safe to copy chunks to be freely
reordered throughout the entire file. However, unless the wheel file is empty,
we know it's going to have at least one ``DATA`` chunk, those ``DATA`` chunks
are likely going to be the largest chunks in a wheel, and maintaining relative
position to them allows storing chunks that you'd really rather read before you
get to all of the ``DATA`` before the ``DATA`` portions.

We otherwise don't allow depending on ordering of optional chunks relative to
other optional chunks, because otherwise it wouldn't be possible to insert new
optional chunks at all if there were any unknown chunk types in the file.


Backwards Compatibility
=======================


Security Implications
=====================


How to Teach This
=================


Reference Implementation
========================


Rejected Ideas
==============


Open Issues
===========

Magic Number "Name"
-------------------

As far as I know, there are no other file types that begin with ``b"\x9bP"``,
however that means that our 2 byte discriminator includes ``P`` because our
ASCII tag is ``PYWHL``, and if we ever want to extend this to sdists, then we
couldn't use ``PYSDIST`` without having the same first two bytes.

I'm not sure how many systems out there use the first two bytes (PNG just
mentions it but doesn't indicate which systems those are) and whether or not we
care.

We could just shorten the name to be ``WHL`` which saves 2 bytes and fixes the
problem, but I kinda liked that ``PYWHL`` disambiguates this from any other
kind of wheel... but I'm not aware of any other kind of wheel and our file
extension is ``.whl`` so it's probably fine? Then if we ever did something like
this for sdist it could be ``SDIST``, though that feels overlly generic, but
isn't something we'd have to fix today?

We could also do ``WHLPY`` or something, but that reads "wrong" to me.


Checksum Algorithm
------------------

The PNG format uses CRC, which is old and tried/true and already has fast-ish
code in the Python standard library to compute it.

That being said, CRC32 only provides 32 bits of collision resistance which
means that an attacker could construct malformed wheel files that collide in
interesting ways, and
`CRC32 isn't particularly fast <https://jolynch.github.io/posts/use_fast_data_algorithms/>`__.

A better, more modern option if we want to stick with non-cryptographic hashes
would be `xxhash <https://xxhash.com/>`__, (specifically XXH3 would be ideal)
however that doesn't exist in the Python stdlib so we'd need either a pure
Python option that is "fast enough" for pip and/or we'd need to get it into the
Python standard library. That might be hard since there's not a generic
interface for adding non-crytographic hashes to the stdlib (crc32 is brought in
as part of the ``zlib`` library).

Another option would be to use a crytographic hash to provide more resistance
to tampering. This is, in theory, slower, but modern hash algorithms can be
pretty fast (likely faster than CRC32 implemented in Python, but we should
benchmark it). Something like `blake3 <https://github.com/BLAKE3-team/BLAKE3/>`__
would be ideal, but it's not guaranteed to exist in the stdlib ``hashlib``
so we would need to get it included for pip. The `blake2 <https://www.blake2.net/>`__
algorithm is guaranteed in ``hashlib`` so that *could* be used, but blake3 would
be better.

Personally, I think blake3 seems like a really good choice here. In theory we
don't *need* a crytographic hash, as this checksum is mostly for detecting
transfer issues or bitrot, but stronger hashes makes it harder for attackers to
attack systems by creating polyglot or other types of filr format attacks.


Checksum Chaining
-----------------

The PNG format only implements checksumming for individual chunks, which means
it won't detect issues where entire chunks are missing or out of order (unless
the ordering is wholly invalid in the spec).

One option to strengthen this would be to compute the checksum by chaining them
or computing a "rolling" checksum.

A rolling checksum would take advantage of the fact that checksumming allows you
to get the current checksum, without throwing away the internal state so that
you can continue to add additional data to it over time. Meaning that each
chunk's checksum is actually a checksum of ``TYPE DATA`` for itself *and* all
previous chunks.

The downside to that would be that it's impossible to fully skip a chunk, as
you'd *have* to read the ``TYPE`` and ``DATA`` bytes and feed them into the
checksumming algorithm in order to properly have the correct state.

The upside is it makes computing the checksums for the entire file more
ergonomic as you're only dealing with a single checksum state, you're just
essentially emitting check points at each chunk.

Chaining checksums is similiar, except instead of having the checksum be the
content of all of the chunks ``TYPE DATA`` up to that point, each chunk is still
checksummed mostly independently, but it is seeded in some way (typically by
using a keyed hash) by the checksum of the previous chunk.

The downside to this is computing the checksum for the whole file is a little
more complicated and less ergonomic and you're limited either to algorithms that
support keyed hashing or you have to use HMAC or invent your own scheme.

The upside is that skipping chunks without reading the bytes still mostly works,
you just have to also read the ``CHECKSUM`` bytes from the previous chunk.

Personally I think we should implement something to extend the checksum across
all chunks, as that makes it much harder to tamper or produce malformed files
and it also has the really nice property that the checksum on the final chunk
also acts as a checksum for the entire file (mostly, it doesn't include the
lengths but that should be fine as if the lengths are wrong the checksum will
still fail).

I lean towards using chaining, particularly if we end up using blake3 or another
hash that natively supports keyed hashing, since that still allows for
efficiently skipping blocks.


Structure of Data
=================

.. note:: TODO: If we change this, we might have to change the ordering rules since
          they assumed a ``DATA`` chunk type.

We can structure records such that there's a "File Record" chunk that points to
a "File Data" chunk, which allows us to build a sort of index of files at the
front (or end if we want) of the archive.

This makes it easy for us to tweak things so that files can either be compressed
together or individually. However, it makes it harder to compress the file
metadata itself (it can be done, but will have less compression than if the
compression operates over everything).

It also makes chunk ordering more important, as all of the file records have to
come first before the data (orwe need to implement seeking around) and that can
cause problems, but it does make getting a list of files much faster.

We could have individual chunks that hold file metadata + file data, one per file,
which keeps each chunk self contained, but means that we're back at individual
compression unless we come up with a scheme that allows chaining multiple chunks
together for compression.

This could function similiarly to the "rolling checksum" option, where you have
to

Another option besides chaining multiple files together, could be a dictionary
chunk that allows sharing a compression dictionary for multiple files.


Acknowledgements
================


Appendix: Zip Format
====================

A basic explainer of the zip file format is that it contains a stream of zero or
more file entries, each one containing a "local file header" and the actual file
data. After all of the file entries is a "central record", which consists of a
stream of zero or more "central directory records" and then ends with a "end of
central directory record".

It roughly looks like:

.. image:: pep-9999/ZIP-64_Internal_Layout.svg
   :alt: An overview of the zip file structure, showing the multiple file
         entries with related local header, and central directory with each
         central directory record.

It's not possible to unambigously determine whether a given file is a zip file
or not, and different parsers will give different answers. This is because the
only way to determine whether a file is a zip file, is to search backwards from
the end of the file until you locate the magic byte sequence that identifies the
"End of Central Directory" and go from there.

This can be easy in the common case where the magic byte sequence is 22 bytes
away from the end of the file.

Unfortunately the End of Central Directory Record includes, as the last item in
the record, a variable sized "comment" field which can store arbitrary bytes,
including the same magic bytes that we're traversing backwards looking for, and
since this comment field is variable sized, we don't know how far we actually
need to traverse.

The zip format
`spec <https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT>`__
recommends not traversing more than 64k bytes, but does not require that, and
different parsers have opted to place a different, or even no, limit on how far
they will traverse.

File streams are not setup to be easily read in reverse.

In the ideal case you'll have the ability to seek randomly throughout the
stream, but even then you have to guess where to seek to, and then read forwards
from there and hope that the magic bytes are contained there, otherwise you'll
need to jump further back and try again.

In the less than ideal case, you'll only have the most basic stream
functionality and you can only read the stream "in order", and if you want to
re-read content that you've already seen you'll have to buffer it yourself.

Python's ``zipfile`` module will first seek to the end to get file size, then
seek 22 bytes from the end of the file, and if that doesn't contain the magic
bytes, will then seek 65557 bytes from the end of the file (or to the start of
the file if it's less than that), and then read 65557 bytes and look for the
magic bytes from there. It does not support streams that are not seekable.

Other parsers have different behaviors, including some parsers which will ignore
the central directory altogether and read the zip file from front to back and
rely only on the "local headers" (common for parsers that support streaming
unzip).

This makes it very easy to construct a zip file that contains very different
content based on the specific parser that is being used to read it.

This "from the back" design of zip also means that, along with being able to
add arbitrary bytes to the *end* of a zip file, you can also add arbitrary bytes
to the *front* of a zip file. This can be a useful trick that is often used to
implement things like a appending a zip file to a binary that extracts itself,
or to Python that can add itself to ``sys.path`` and use zip imports.

It also makes it really easy to create polyglot files (which is what the self
extracting zip file is), which can be used by a malicous actor to smuggle files
past security scanners or other checks that respond differently to different
types of files.

Zip files also support the concept of updating or deleting a file that has
already been written to the stream, without having to go back and rewrite the
already written data, by abandoning the previously written data to be ignored
and writing a duplicate file entry with the updated data and only refencing that
duplicate file entry from the central directory (or omitting it from the central
record for deletion). This made sense nearly 40 years ago when you might be
writing a zip file that spanned across multiple floppy disks, where having to go
back and restart the stream from the beginning could be an onerous ask.

In the modern world however, rewriting a zip file is typically reasonable, and
this feature adds another way for parsers to arrive at different results from
the same zip file (particularly streaming zip vs seekable zip).

The zip file format also used fixed size integers throughout the spec, and due
to its age, these fixed size integers are all 32bits or less, including for
values like file size or relative offset to a file pointer, giving a zip file
about a 4GB maximum size, as well as a 4GB maximum size for individual files in
the zip, before and after compression. To allow larger sizes than that, there
are "Zip64" records that can be added to a zip file, duplicating information
from other parts of the zip file format, but with larger fixed size integers.

Zip files have about a minimum of 100 bytes of overhead per file added to the
archive (in addition to any other overhead from the base zip format structure
itself), and these Zip64 duplicate records adds another 64 bytes of possible
overhead. Since these Zip64 records are duplicating information that exists
elsehere, parsers that are not setup to read Zip64 can get different results
from reading a zip file because they won't see the Zip64 records.

The method used to extend the zip format for Zip64 is a generic method that
allows adding extra "fields" of data to Zip records, and this mechanism is
available for anyone who wants to extend the zip file format in some way, and
was used by many different implementations to extend zip files or work around
some issue with them.

Of course support for these various extensions to the zip file format is not
consistent among parsers, some of them are very commonly supported (such as
Zip64) and others are essentially not supported. When a parser doesn't
understand one of these extensions, there's no clear specification for what they
should do, whther it's safe to ignore them or not. These extensions can also be
repeated within the same entry, which may or may not make sense (it does not
make sense for any of the known public extensions), with again, no guidance on
what a parser should do in the face of repeated extensions.

This all leads again to different parsers seeing different results from the same
zip file.

As an example, Python's ``zipfile`` supports only two of the extensions, Zip64
and Unicode Filenames. It will silently ignore any unknown extensions (other
than storing them for the end user to access them). When seeing the same
extension repeated, it will use the *first* instance for the Zip64 extension,
but the *last* instance for the Unicode Path extension. If the data that the
Zip64 extension duplicates from the base zip format isn't set to a specific
value (the max value for that field) it will silently ignore the Zip64
extension.

These extensions, given they are often trying to make up for an issue with the
"base" zip file format, sometimes carry duplicate information from the base
spec or even from each other. They can also be used repeatedly, even for cases
where that doesn't make sense, and how a parser handles that also varies between
the different parsers.

This issue of the same data being duplicated occurs across several fields in
zip files, and wherever it occurs it's not well specified which values should be
considered authoritative, and in practice varies between parsers, giving
different parsers different answers if that duplicated data isn't ensured to
match up. For example, The "filename" for a single file can occur anywhere
between 2 and 5 times (technically unbounded since that includes extensions that
could be repeated) and different parsers are *known* to treat different sources
authoritatively.

Zip files support compression, but due to trade offs made this compression is
only capable of compressing a single file at a time rather than multiple files
together. Compressing files one at a time allows for more efficient random
access of files, since you only need to decompress the bytes for the file you
actually need, but means that your compression will be less efficient.

Whether random access or higher compression ratios is more important will depend
greatly on the use case, and there isn't a singular right answer for a general
purpose file format, but single file compression conceptually fits into the
design of zip better, so it chose that. For the Wheel use case however, random
access isn't a particularly relevant feature to optimize since the majority of
time you'll want the entire contents of the wheel, other than specifically for
the ``METADATA`` file for resolvers.

While zip files support compression for file data, they do *not* support
compression for any of the overhead associated with the zip file format, even
though this data would be highly compressible given how much of that overhead is
storing duplicate copies of the same data.

To top it off, given its age, the zip file format has more minor cruft spread
all over it that, while it often isn't directly harmful, either adds overhead or
has caused people to engineer workarounds that do end up being harmful. Some of
that minor cruft is:

* All of the timestamps in the base zip format are stored as MS-DOS time, which
  only has a 2 second resolution and no timezone which also cannot represent
  times before 1980. Several extensions have been added that include
  duplications of these timestamps with varying levels of precision.
* Every file record in the central directory uses 2 bytes to record which floppy
  disk number the actual file data is stored on and the central directory stores
  how many total disks this zip file spans. This has since been repurposed into
  allowing splitting a zip file into multiple segments instead of across
  multiple disks.
* When splitting a zip file up, the spec says that PKZip will use a magic number
  at offset 0, but does not require or mention if it is expected that other
  implementations do that. They document that the magic number they've chosen
  collides with magic bytes used by some implementations in other parts of a
  zip file.
* The zip file format has both a concept of the version needed to *extract* and
  the version that *created* the zip file, some parsers will ignore this, others
  will not.
* The zip file format stores what OS and/or FS (it mixes the concepts) a zip
  file was made on, based on a fixed list of both OSs and FSs.
* There are "comment" fields throughout, which makes it very easy for even well
  formed zip files to smuggle information around or create polyglot files.
* One of the places data can be stored at uses 32 bit fixed integers, which
  just automatically grow to support 64 bit integers if Zip64 is being used, but
  Zip64 isn't a boolean so it's unclear how a parser is supposed to know.
* One of the types of records is originally defined as not having any sort of
  signature to identify it, but when it's used it **must** be used immediately
  after the file data. It's documented in the spec that it's been common for
  implementors to prefix that record with a set of magic bytes to identify it
  (there by breaking the spec). The spec has now been updated to say that you
  have to be able to handle both with and without the magic bytes, and you
  should emit the magic bytes.
* Zip files technically support many different kidns of compression, some of
  which are specific to zip files and rarely are supported.
* Zip files support encryption (and oddly compressing the central directory is
  supported when encryption is in use).
* Zip files have support for "patch files", where you can update a previous
  file by saving a patch to that file rather than duplicating the entire stream,
  but I'm not aware of anyone using it, and it has a note that the feature can
  only be used by PKZip and is covered under patents.
* Textual fields in Zip files are, by spec, encoded using cp437 (a hold over
  from MS-DOS), however some implementations would write other encodings into
  those fields. There are common extensions that can be used (duplicating the
  data of course) to encode that same data using utf8 instead. The zip file
  format has also grown a boolean flag to flip all textual fields in the file to
  use utf8 encoding.
* There's an extension field that has been "reserved" for indicating the
  encoding of the filename specifically, but the spec has not indicated how that
  field should actually be used and the field is explicitly undefined. The spec
  also suggests that this field could be used somehow to specify the encoding of
  the file itself as well as the decryption passsword.
* There's various file attributes stored for every file that are specific to
  MS-DOS or other older or no longer used systems.
* Most of the extensions are used to store extended attributes that are relevant
  only to a specific operating system or filesystem, some of which are not
  portable between machines.
* The zip spec recommends that any application specific manifest files be placed
  as the first file in the zip file, but the Wheel 1.0 recommends it should be
  the last file.


Footnotes
=========


Copyright
=========

This document is placed in the public domain or under the
CC0-1.0-Universal license, whichever is more permissive.
